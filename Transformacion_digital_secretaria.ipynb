{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.163.0)\n",
      "Requirement already satisfied: google-auth-httplib2 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: gspread in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.2.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (2.38.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (2.24.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth-oauthlib) (2.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.69.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (5.29.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pablo\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pablo\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\1501271018.py:26: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  rn=int(RN_df.loc[:,\"TablaFinImpacto\"])-3\n"
     ]
    }
   ],
   "source": [
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "\n",
    "scopes = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\"\n",
    "]\n",
    "\n",
    "creds = Credentials.from_service_account_file(\"credentials.json\", scopes= scopes)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn=int(RN_df.loc[:,\"TablaFinImpacto\"])-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\1449633345.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Respuesta'] = df_final['Respuesta'].apply(quitar_acentos)\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\1449633345.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Respuesta'] = df_final['Respuesta'].astype(str)\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\1449633345.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Respuesta'] = corpus\n"
     ]
    }
   ],
   "source": [
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "\n",
    "scopes = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\"\n",
    "]\n",
    "\n",
    "creds = Credentials.from_service_account_file(\"credentials.json\", scopes= scopes)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "TablaFinImpacto=libro.get_worksheet(7)\n",
    "TablaFinImpacto_d = TablaFinImpacto.get_all_values()\n",
    "TablaFinImpacto_df=pd.DataFrame(TablaFinImpacto_d)\n",
    "TablaFinImpacto_df=TablaFinImpacto_df.iloc[2:,[0,4]]\n",
    "TablaFinImpacto_df.columns=[\"ID\", \"Respuesta\"]\n",
    "df_final = TablaFinImpacto_df.iloc[rn:,:]\n",
    "\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "df_final['Respuesta'] = df_final['Respuesta'].apply(quitar_acentos)\n",
    "df_final['Respuesta'] = df_final['Respuesta'].astype(str)\n",
    "nltk.download('all')\n",
    "text = list(df_final['Respuesta'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "\n",
    "df_final['Respuesta'] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\264407527.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final[\"Clasificación\"]=prediccion_f\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\264407527.py:33: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  Puntaje.update(f\"A{last_row}\", values)\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\264407527.py:34: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  nuevo_valor=int(RN_df.loc[:,\"TablaFinImpacto\"]) + len(pred_s)\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\264407527.py:35: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  RN.update(\"G2\", [[nuevo_valor]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY',\n",
       " 'updatedRange': 'Row_number!G2',\n",
       " 'updatedRows': 1,\n",
       " 'updatedColumns': 1,\n",
       " 'updatedCells': 1}"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (\"NBbuenoTD1.pkl\",\"rb\") as modelo_1:\n",
    "    modelo_td1 = pickle.load(modelo_1)\n",
    "\n",
    "prediccion = modelo_td1.predict(df_final['Respuesta'])\n",
    "prediccion_f = prediccion + 2\n",
    "df_final[\"Clasificación\"]=prediccion_f\n",
    "\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[0,1]]\n",
    "Rubrica_df.columns=[\"Mision\", \"Clasificación\"]\n",
    "Rubrica_df[\"Mision\"]=Rubrica_df[\"Mision\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s=pd.merge(df_final,Rubrica_df, on='Clasificación', how='inner')\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.67,\n",
    "    2: 3.33,\n",
    "    3: 5.00,\n",
    "    4: 6.67,\n",
    "    5: 8.33,\n",
    "    6: 10\n",
    "}\n",
    "pred_s[\"Puntaje\"] = pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[0,-2,-1]]\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "last_row = len(Puntaje.get_all_values()) + 1  # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"A{last_row}\", values)\n",
    "nuevo_valor=int(RN_df.loc[:,\"TablaFinImpacto\"]) + len(pred_s)\n",
    "RN.update(\"G2\", [[nuevo_valor]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherencia del diseño programático"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\171902243.py:10: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n"
     ]
    }
   ],
   "source": [
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
    "print(rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:36: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\471732412.py:36: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df_final[\"Duración\"] = df_final[\"Duración\"].str.extract(\"(\\d+)\")  # Extrae solo los números\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\471732412.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Diseno'] = df_final['Diseno'].apply(quitar_acentos)\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\471732412.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Diseno'] = df_final['Diseno'].astype(str)\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\471732412.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Contribucion'] = df_final['Contribucion'].apply(quitar_acentos)\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\471732412.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Contribucion'] = df_final['Contribucion'].astype(str)\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\471732412.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final[\"Duración\"] = df_final[\"Duración\"].str.extract(\"(\\d+)\")  # Extrae solo los números\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\471732412.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final[\"Duración\"] = pd.to_numeric(df_final[\"Duración\"])\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\471732412.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Diseno'] = corpus\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\471732412.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Contribucion'] = corpus\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Diseno</th>\n",
       "      <th>Duración</th>\n",
       "      <th>Contribucion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>7076548</td>\n",
       "      <td>primero realizamos diagnostico participativo e...</td>\n",
       "      <td>6</td>\n",
       "      <td>intervenciones proyecto atencion medica integr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                             Diseno  Duración  \\\n",
       "234  7076548  primero realizamos diagnostico participativo e...         6   \n",
       "\n",
       "                                          Contribucion  \n",
       "234  intervenciones proyecto atencion medica integr...  "
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "libro = client.open_by_key(\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\") #Original\n",
    "SolicitudDeInversiónSocialFocal=libro.get_worksheet(1)\n",
    "SolicitudDeInversiónSocialFocal_d = SolicitudDeInversiónSocialFocal.get_all_values()\n",
    "SolicitudDeInversiónSocialFocal_df=pd.DataFrame(SolicitudDeInversiónSocialFocal_d)\n",
    "SolicitudDeInversiónSocialFocal_df.columns=SolicitudDeInversiónSocialFocal_df.iloc[0,:]\n",
    "SolicitudDeInversiónSocialFocal_df=SolicitudDeInversiónSocialFocal_df.iloc[1:,:]\n",
    "df=SolicitudDeInversiónSocialFocal_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"DiseñoProyecto\",\"DuraciónProyectoMeses\",\"ContribucionIntervencionFin\"]]\n",
    "df.columns=[\"ID\",\"Diseno\",\"Duración\",\"Contribucion\"]\n",
    "df_final=df.iloc[rn:,:]\n",
    "\n",
    "\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "# df_sfinal = pd.merge(Puntaje_df,Rubrica_df, on='Coherencia', how='inner')\n",
    "# df_final= pd.merge(df_sfinal,df, on=\"ID\", how=\"inner\")\n",
    "# df_final = df_final.iloc[:,[0,3,4,5,2]]\n",
    "df_final['Diseno'] = df_final['Diseno'].apply(quitar_acentos)\n",
    "df_final['Diseno'] = df_final['Diseno'].astype(str)\n",
    "df_final['Contribucion'] = df_final['Contribucion'].apply(quitar_acentos)\n",
    "df_final['Contribucion'] = df_final['Contribucion'].astype(str)\n",
    "df_final[\"Duración\"] = df_final[\"Duración\"].str.extract(\"(\\d+)\")  # Extrae solo los números\n",
    "df_final[\"Duración\"] = pd.to_numeric(df_final[\"Duración\"])\n",
    "# df_final = df_final.dropna()\n",
    "text = list(df_final['Diseno'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Diseno'] = corpus\n",
    "text = list(df_final['Contribucion'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Contribucion'] = corpus\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\1081211086.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final[\"Clasificación\"]= prediccion\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\1081211086.py:57: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  Puntaje.update(f\"D{last_row-len(pred_s)}\", values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y',\n",
       " 'updatedRange': 'Sheet1!D277:E277',\n",
       " 'updatedRows': 1,\n",
       " 'updatedColumns': 2,\n",
       " 'updatedCells': 2}"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"tfidf1_model_td2.pkl\", \"rb\") as f:  \n",
    "    tfidf1_td2_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf2_model_td2.pkl\", \"rb\") as f:  \n",
    "    tfidf2_td2_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"scaler_model_td2.pkl\", \"rb\") as f:  \n",
    "    scaler_td2_loaded = pickle.load(f)  \n",
    "\n",
    "X_text1 = df_final[\"Diseno\"]\n",
    "X_text2 = df_final[\"Contribucion\"]\n",
    "X_num = df_final[[\"Duración\"]]\n",
    "\n",
    "X_text1_t = tfidf1_td2_loaded.transform(X_text1)\n",
    "X_text2_t = tfidf2_td2_loaded.transform(X_text2)\n",
    "X_num_t = scaler_td2_loaded.transform(X_num)\n",
    "\n",
    "X_combined = hstack([X_text1_t, X_text2_t, X_num_t])\n",
    "\n",
    "with open (\"RFbuenoTD2.pkl\",\"rb\") as modelo_2:\n",
    "    modelo_td2 = pickle.load(modelo_2)\n",
    "\n",
    "prediccion = modelo_td2.predict(X_combined)\n",
    "df_final[\"Clasificación\"]= prediccion\n",
    "df_final\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[2,3]]\n",
    "Rubrica_df.columns=[\"Coherencia\", \"Clasificación\"]\n",
    "Rubrica_df[\"Coherencia\"]=Rubrica_df[\"Coherencia\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s=pd.merge(df_final,Rubrica_df, on='Clasificación', how='inner')\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.67,\n",
    "    2: 3.33,\n",
    "    3: 5.00,\n",
    "    4: 6.67,\n",
    "    5: 8.33,\n",
    "    6: 10\n",
    "}\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "Puntaje_d = Puntaje.get_all_values()\n",
    "Puntaje_df=pd.DataFrame(Puntaje_d)\n",
    "Puntaje_df=Puntaje_df.iloc[2:,[0,3]]\n",
    "Puntaje_df.columns=[\"ID\",\"Coherencia\"]\n",
    "pred_s[\"Puntaje\"] = pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[-2,-1]]\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "last_row = len(Puntaje.get_all_values()) + 1 # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"D{last_row-len(pred_s)}\", values)\n",
    "# nuevo_valor=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"]) + len(pred_s)\n",
    "# RN.update(\"A2\", [[nuevo_valor]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema derecho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\1068380983.py:10: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  rn1=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\1068380983.py:11: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  rn2=int(RN_df.loc[:,\"TablaCausas\"])-3\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\1068380983.py:12: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  rn3=int(RN_df.loc[:,\"TablaEfectos\"])-3\n"
     ]
    }
   ],
   "source": [
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn1=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
    "rn2=int(RN_df.loc[:,\"TablaCausas\"])-3\n",
    "rn3=int(RN_df.loc[:,\"TablaEfectos\"])-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "libro = client.open_by_key(\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\") #Original\n",
    "TablaCausas=libro.get_worksheet(9)\n",
    "TablaEfectos=libro.get_worksheet(8)\n",
    "SolicitudDeInversiónSocialFocal=libro.get_worksheet(1)\n",
    "SolicitudDeInversiónSocialFocal_d = SolicitudDeInversiónSocialFocal.get_all_values()\n",
    "SolicitudDeInversiónSocialFocal_df=pd.DataFrame(SolicitudDeInversiónSocialFocal_d)\n",
    "SolicitudDeInversiónSocialFocal_df.columns=SolicitudDeInversiónSocialFocal_df.iloc[0,:]\n",
    "SolicitudDeInversiónSocialFocal_df=SolicitudDeInversiónSocialFocal_df.iloc[1:,:]\n",
    "df=SolicitudDeInversiónSocialFocal_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"PrincipalDerechoHumano\",\"ProblemaPrincipal\",\"DatosCausas\",\"QuéPasaríaConLasPersonasQueAtiendeSiSuOrganizaciónNoInterviniera\"]]\n",
    "df.columns=[\"ID\",\"DerechoHumano\",\"Problema\",\"DatosCausas\",\"NoIntervencion\"]\n",
    "df=df.iloc[rn:,:]\n",
    "TablaCausas=libro.get_worksheet(9)\n",
    "TablaCausas_d = TablaCausas.get_all_values()\n",
    "TablaCausas_df=pd.DataFrame(TablaCausas_d)\n",
    "TablaCausas_df.columns=TablaCausas_df.iloc[0,:]\n",
    "TablaCausas_df=TablaCausas_df.iloc[2:,:]\n",
    "TablaCausas_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"Causas\"]]\n",
    "TablaCausas_df[\"Causas\"] = TablaCausas_df[\"Causas\"].str.replace('\\n', ' ', regex=True)\n",
    "df2=TablaCausas_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"Causas\"]]\n",
    "df2.columns=[\"ID\",\"Causas\"]\n",
    "df2=df2.iloc[rn2:,:]\n",
    "TablaEfectos=libro.get_worksheet(8)\n",
    "TablaEfectos_d = TablaEfectos.get_all_values()\n",
    "TablaEfectos_df=pd.DataFrame(TablaEfectos_d)\n",
    "TablaEfectos_df.columns=TablaEfectos_df.iloc[0,:]\n",
    "TablaEfectos_df=TablaEfectos_df.iloc[2:,:]\n",
    "TablaEfectos_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"Efectos\"]]\n",
    "TablaEfectos_df[\"Efectos\"] = TablaEfectos_df[\"Efectos\"].str.replace('\\n', ' ', regex=True)\n",
    "df3=TablaEfectos_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"Efectos\"]]\n",
    "df3.columns=[\"ID\",\"Efectos\"]\n",
    "df3=df3.iloc[rn3:,:]\n",
    "df_2= pd.merge(df,df2, on=\"ID\", how=\"inner\")\n",
    "df_final= pd.merge(df_2,df3, on=\"ID\", how=\"inner\")\n",
    "\n",
    "\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "df_final['DerechoHumano'] = df_final['DerechoHumano'].apply(quitar_acentos)\n",
    "df_final['DerechoHumano'] = df_final['DerechoHumano'].astype(str)\n",
    "df_final['Problema'] = df_final['Problema'].apply(quitar_acentos)\n",
    "df_final['Problema'] = df_final['Problema'].astype(str)\n",
    "df_final['DatosCausas'] = df_final['DatosCausas'].apply(quitar_acentos)\n",
    "df_final['DatosCausas'] = df_final['DatosCausas'].astype(str)\n",
    "df_final['NoIntervencion'] = df_final['NoIntervencion'].apply(quitar_acentos)\n",
    "df_final['NoIntervencion'] = df_final['NoIntervencion'].astype(str)\n",
    "df_final['Causas'] = df_final['Causas'].apply(quitar_acentos)\n",
    "df_final['Causas'] = df_final['Causas'].astype(str)\n",
    "df_final['Efectos'] = df_final['Efectos'].apply(quitar_acentos)\n",
    "df_final['Efectos'] = df_final['Efectos'].astype(str)\n",
    "\n",
    "text = list(df_final['DerechoHumano'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['DerechoHumano'] = corpus\n",
    "text = list(df_final['Problema'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Problema'] = corpus\n",
    "text = list(df_final['DatosCausas'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['DatosCausas'] = corpus\n",
    "text = list(df_final['NoIntervencion'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['NoIntervencion'] = corpus\n",
    "text = list(df_final['Causas'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Causas'] = corpus\n",
    "text = list(df_final['Efectos'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Efectos'] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\3176429033.py:73: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  Puntaje.update(f\"G{last_row-len(pred_s)}\", values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y',\n",
       " 'updatedRange': 'Sheet1!G277:H277',\n",
       " 'updatedRows': 1,\n",
       " 'updatedColumns': 2,\n",
       " 'updatedCells': 2}"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"tfidf1_model_td3.pkl\", \"rb\") as f:  \n",
    "    tfidf1_td3_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf2_model_td3.pkl\", \"rb\") as f:  \n",
    "    tfidf2_td3_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf3_model_td3.pkl\", \"rb\") as f:  \n",
    "    tfidf3_td3_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf4_model_td3.pkl\", \"rb\") as f:  \n",
    "    tfidf4_td3_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf5_model_td3.pkl\", \"rb\") as f:  \n",
    "    tfidf5_td3_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf6_model_td3.pkl\", \"rb\") as f:  \n",
    "    tfidf6_td3_loaded = pickle.load(f)  \n",
    "\n",
    "\n",
    "X_text1=df_final['DerechoHumano'] = df_final['DerechoHumano'].apply(quitar_acentos)\n",
    "X_text2=df_final['Problema'] = df_final['Problema'].apply(quitar_acentos)\n",
    "X_text3=df_final['DatosCausas'] = df_final['DatosCausas'].apply(quitar_acentos)\n",
    "X_text4=df_final['NoIntervencion'] = df_final['NoIntervencion'].apply(quitar_acentos)\n",
    "X_text5=df_final['Causas'] = df_final['Causas'].apply(quitar_acentos)\n",
    "X_text6=df_final['Efectos'] = df_final['Efectos'].apply(quitar_acentos)\n",
    "\n",
    "X_text1_t=tfidf1_td3_loaded.transform(X_text1)\n",
    "X_text2_t=tfidf2_td3_loaded.transform(X_text2)\n",
    "X_text3_t=tfidf3_td3_loaded.transform(X_text3)\n",
    "X_text4_t=tfidf4_td3_loaded.transform(X_text4)\n",
    "X_text5_t=tfidf5_td3_loaded.transform(X_text5)\n",
    "X_text6_t=tfidf6_td3_loaded.transform(X_text6)\n",
    "\n",
    "X_combined = hstack([X_text1_t, X_text2_t, X_text3_t, X_text4_t, X_text5_t, X_text6_t])\n",
    "\n",
    "with open (\"RFbuenoTD3.pkl\",\"rb\") as modelo_3:\n",
    "    modelo_td3 = pickle.load(modelo_3)\n",
    "\n",
    "prediccion = modelo_td3.predict(X_combined)\n",
    "df_final[\"Clasificación\"]= prediccion\n",
    "\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[4,5]]\n",
    "Rubrica_df.columns=[\"ProblemaDerecho\", \"Clasificación\"]\n",
    "Rubrica_df[\"ProblemaDerecho\"]=Rubrica_df[\"ProblemaDerecho\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s=pd.merge(df_final,Rubrica_df, on='Clasificación', how='inner')\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.17,\n",
    "    2: 2.33,\n",
    "    3: 3.50,\n",
    "    4: 4.67,\n",
    "    5: 5.83,\n",
    "    6: 7\n",
    "}\n",
    "\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "Puntaje_d = Puntaje.get_all_values()\n",
    "Puntaje_df=pd.DataFrame(Puntaje_d)\n",
    "Puntaje_df=Puntaje_df.iloc[2:,[0,6]]\n",
    "Puntaje_df.columns=[\"ID\",\"ProblemaDerecho\"]\n",
    "pred_s[\"Puntaje\"] = pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[-2,-1]]\n",
    "pred_s\n",
    "last_row = len(Puntaje.get_all_values()) + 1 # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"G{last_row-len(pred_s)}\", values)\n",
    "\n",
    "# # df_sfinal = pd.merge(Puntaje_df,Rubrica_df, on='ProblemaDerecho', how='inner')\n",
    "# # df_final= pd.merge(df_sfinal,df_3, on=\"ID\", how=\"inner\")\n",
    "# # df_final = df_final.iloc[:,[0,3,4,5,6,7,8,2]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspectiva de atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\2479684632.py:10: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
    "rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\3647332631.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Respuesta'] = df_final['Respuesta'].apply(quitar_acentos)\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\3647332631.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Respuesta'] = df_final['Respuesta'].astype(str)\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\3647332631.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Respuesta'] = df_final['Respuesta'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\pablo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\3647332631.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['Respuesta'] = corpus\n"
     ]
    }
   ],
   "source": [
    "libro = client.open_by_key(\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\") #Original\n",
    "SolicitudDeInversiónSocialFocal=libro.get_worksheet(1)\n",
    "SolicitudDeInversiónSocialFocal_d = SolicitudDeInversiónSocialFocal.get_all_values()\n",
    "SolicitudDeInversiónSocialFocal_df=pd.DataFrame(SolicitudDeInversiónSocialFocal_d)\n",
    "SolicitudDeInversiónSocialFocal_df.columns=SolicitudDeInversiónSocialFocal_df.iloc[0,:]\n",
    "SolicitudDeInversiónSocialFocal_df=SolicitudDeInversiónSocialFocal_df.iloc[1:,:]\n",
    "SolicitudDeInversiónSocialFocal_df=SolicitudDeInversiónSocialFocal_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"SusActividadesImplicanElInvolucramientoDeLosTitularesDeResponsabilidadesfamiliasComunidadesOrganizacionesDeLaSociedadCivilOSCSindicatosYDeObligacionesinstitucionesPúblicasDelEstadoYLosGobernantesQuéSeRealizaConEllosasQuéBuscaGenerarEnLosMismos\"]]\n",
    "SolicitudDeInversiónSocialFocal_df.columns=[\"ID\", \"Respuesta\"]\n",
    "df_final=SolicitudDeInversiónSocialFocal_df.iloc[rn:,:]\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "# df_sfinal = pd.merge(Puntaje_df,Rubrica_df, on='Perspectiva', how='inner')\n",
    "# df_final= pd.merge(df_sfinal,SolicitudDeInversiónSocialFocal_df, on=\"ID\", how=\"inner\")\n",
    "# df_final=df_final.iloc[:,[-1,-2]]\n",
    "df_final['Respuesta'] = df_final['Respuesta'].apply(quitar_acentos)\n",
    "df_final['Respuesta'] = df_final['Respuesta'].astype(str)\n",
    "df_final['Respuesta'] = df_final['Respuesta'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "nltk.download('all')\n",
    "text = list(df_final['Respuesta'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Respuesta'] = corpus\n",
    "\n",
    "X = df_final['Respuesta']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\1733655343.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final[\"Clasificación\"]= prediccion\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\1733655343.py:35: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
      "  Puntaje.update(f\"I{last_row-len(pred_s)}\", values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y',\n",
       " 'updatedRange': 'Sheet1!I277:J277',\n",
       " 'updatedRows': 1,\n",
       " 'updatedColumns': 2,\n",
       " 'updatedCells': 2}"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open (\"RF_TD4.pkl\",\"rb\") as modelo_4:\n",
    "    modelo_td4 = pickle.load(modelo_4)\n",
    "\n",
    "prediccion = modelo_td4.predict(X)\n",
    "df_final[\"Clasificación\"]= prediccion\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[6,7]]\n",
    "Rubrica_df.columns=[\"Perspectiva\", \"Clasificación\"]\n",
    "Rubrica_df[\"Perspectiva\"]=Rubrica_df[\"Perspectiva\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s=pd.merge(df_final,Rubrica_df, on='Clasificación', how='inner')\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.08,\n",
    "    2: 2.17,\n",
    "    3: 3.25,\n",
    "    4: 4.33,\n",
    "    5: 5.42,\n",
    "    6: 6.50\n",
    "}\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "Puntaje_d = Puntaje.get_all_values()\n",
    "Puntaje_df=pd.DataFrame(Puntaje_d)\n",
    "Puntaje_df=Puntaje_df.iloc[2:,[0,8]]\n",
    "Puntaje_df.columns=[\"ID\",\"Perspectiva\"]\n",
    "pred_s[\"Puntaje\"] = pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[-2,-1]]\n",
    "last_row = len(Puntaje.get_all_values()) + 1 # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"I{last_row-len(pred_s)}\", values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Población objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\253484010.py:10: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
      "C:\\Users\\pablo\\AppData\\Local\\Temp\\ipykernel_43760\\253484010.py:11: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  rn2=int(RN_df.loc[:,\"DesgloseDeSujetosDeDerecho\"])-3\n"
     ]
    }
   ],
   "source": [
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
    "rn2=int(RN_df.loc[:,\"DesgloseDeSujetosDeDerecho\"])-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "APIError: [429]: Quota exceeded for quota metric 'Read requests' and limit 'Read requests per minute per user' of service 'sheets.googleapis.com' for consumer 'project_number:172469642267'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[514], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m libro \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_by_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Original\u001b[39;00m\n\u001b[0;32m      2\u001b[0m Desglose\u001b[38;5;241m=\u001b[39mlibro\u001b[38;5;241m.\u001b[39mget_worksheet(\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      3\u001b[0m SolicitudDeInversiónSocialFocal\u001b[38;5;241m=\u001b[39mlibro\u001b[38;5;241m.\u001b[39mget_worksheet(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gspread\\client.py:174\u001b[0m, in \u001b[0;36mClient.open_by_key\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m HTTPStatus\u001b[38;5;241m.\u001b[39mFORBIDDEN:\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mex\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spreadsheet\n",
      "File \u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gspread\\client.py:168\u001b[0m, in \u001b[0;36mClient.open_by_key\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Opens a spreadsheet specified by `key` (a.k.a Spreadsheet ID).\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m:param str key: A key of a spreadsheet as it appears in a URL in a browser.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m>>> gc.open_by_key('0BmgG6nO_6dprdS1MN3d3MkdPa142WFRrdnRRUWl1UFE')\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     spreadsheet \u001b[38;5;241m=\u001b[39m \u001b[43mSpreadsheet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APIError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m HTTPStatus\u001b[38;5;241m.\u001b[39mNOT_FOUND:\n",
      "File \u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gspread\\spreadsheet.py:29\u001b[0m, in \u001b[0;36mSpreadsheet.__init__\u001b[1;34m(self, http_client, properties)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m http_client\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_properties \u001b[38;5;241m=\u001b[39m properties\n\u001b[1;32m---> 29\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_sheet_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_properties\u001b[38;5;241m.\u001b[39mupdate(metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gspread\\spreadsheet.py:230\u001b[0m, in \u001b[0;36mSpreadsheet.fetch_sheet_metadata\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfetch_sheet_metadata\u001b[39m(\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m, params: Optional[ParamsType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    220\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Similar to :method spreadsheets_get:`gspread.http_client.spreadsheets_get`,\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    get the spreadsheet form the API but by default **does not get the cells data**.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    It only retrieve the the metadata from the spreadsheet.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    :rtype: dict\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_sheet_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gspread\\http_client.py:305\u001b[0m, in \u001b[0;36mHTTPClient.fetch_sheet_metadata\u001b[1;34m(self, id, params)\u001b[0m\n\u001b[0;32m    301\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincludeGridData\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    303\u001b[0m url \u001b[38;5;241m=\u001b[39m SPREADSHEET_URL \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mid\u001b[39m\n\u001b[1;32m--> 305\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\pablo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gspread\\http_client.py:128\u001b[0m, in \u001b[0;36mHTTPClient.request\u001b[1;34m(self, method, endpoint, params, data, json, files, headers)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIError(response)\n",
      "\u001b[1;31mAPIError\u001b[0m: APIError: [429]: Quota exceeded for quota metric 'Read requests' and limit 'Read requests per minute per user' of service 'sheets.googleapis.com' for consumer 'project_number:172469642267'."
     ]
    }
   ],
   "source": [
    "libro = client.open_by_key(\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\") #Original\n",
    "Desglose=libro.get_worksheet(4)\n",
    "SolicitudDeInversiónSocialFocal=libro.get_worksheet(1)\n",
    "SolicitudDeInversiónSocialFocal_d = SolicitudDeInversiónSocialFocal.get_all_values()\n",
    "SolicitudDeInversiónSocialFocal_df=pd.DataFrame(SolicitudDeInversiónSocialFocal_d)\n",
    "SolicitudDeInversiónSocialFocal_df.columns=SolicitudDeInversiónSocialFocal_df.iloc[0,:]\n",
    "SolicitudDeInversiónSocialFocal_df=SolicitudDeInversiónSocialFocal_df.iloc[1:,:]\n",
    "df=SolicitudDeInversiónSocialFocal_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"HanRealizadoAlgúnDiagnósticoQueLePermitaConocerEnProfundidadLaSituaciónDeSuPoblaciónObjetivo\"]]\n",
    "df.columns=[\"ID\",\"Estudio\"]\n",
    "df=df.iloc[rn:,:]\n",
    "Desglose_d = Desglose.get_all_values()\n",
    "Desglose_df=pd.DataFrame(Desglose_d)\n",
    "Desglose_df.columns=Desglose_df.iloc[0,:]\n",
    "Desglose_df=Desglose_df.iloc[2:,:]\n",
    "Desglose_df=Desglose_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"Edad\",\"NoHombres\",\"NoMujeres\"]]\n",
    "Desglose_df.columns=[\"ID\",\"Edad\",\"Hombres\",\"Mujeres\"]\n",
    "Desglose_df=Desglose_df.iloc[rn2:,:]\n",
    "df_final= pd.merge(df,Desglose_df, on=\"ID\", how=\"inner\")\n",
    "# df_sfinal = pd.merge(Puntaje_df,Rubrica_df, on='PoblacionObj', how='inner')\n",
    "# df_final= pd.merge(df_sfinal,df_2, on=\"ID\", how=\"inner\")\n",
    "# df_final = df_final.iloc[:,[0,3,4,5,6,2]]\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "df_final['Estudio'] = df_final['Estudio'].apply(quitar_acentos)\n",
    "df_final['Estudio'] = df_final['Estudio'].astype(str)\n",
    "\n",
    "df_final = df_final.dropna()\n",
    "text = list(df_final['Estudio'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Estudio'] = corpus\n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"RF_TD5.pkl\",\"rb\") as modelo_5:\n",
    "    modelo_td5 = pickle.load(modelo_5)\n",
    "\n",
    "X = df_final.loc[:,['Estudio', 'Edad', 'Hombres', 'Mujeres']]\n",
    "\n",
    "prediccion = modelo_td5.predict(X)\n",
    "df_final[\"Clasificación\"]= prediccion\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[8,9]]\n",
    "Rubrica_df.columns=[\"PoblacionObj\",\"Clasificación\"]\n",
    "Rubrica_df[\"PoblacionObj\"]=Rubrica_df[\"PoblacionObj\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s=pd.merge(df_final,Rubrica_df, on='Clasificación', how='inner')\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.08,\n",
    "    2: 2.17,\n",
    "    3: 3.25,\n",
    "    4: 4.33,\n",
    "    5: 5.42,\n",
    "    6: 6.5\n",
    "}\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "Puntaje_d = Puntaje.get_all_values()\n",
    "Puntaje_df=pd.DataFrame(Puntaje_d)\n",
    "Puntaje_df=Puntaje_df.iloc[2:,[0,10]]\n",
    "Puntaje_df.columns=[\"ID\",\"PoblacionObj\"]\n",
    "pred_s[\"Puntaje\"] = pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[-2,-1]]\n",
    "last_row = len(Puntaje.get_all_values()) + 1 # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"K{last_row-len(pred_s)}\", values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación del conocimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
    "rn2=int(RN_df.loc[:,\"Actividades\"])-3\n",
    "print(rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libro = client.open_by_key(\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\") #Original\n",
    "SolicitudDeInversiónSocialFocal=libro.get_worksheet(1)\n",
    "Actividades=libro.get_worksheet(5)\n",
    "SolicitudDeInversiónSocialFocal_d = SolicitudDeInversiónSocialFocal.get_all_values()\n",
    "SolicitudDeInversiónSocialFocal_df=pd.DataFrame(SolicitudDeInversiónSocialFocal_d)\n",
    "SolicitudDeInversiónSocialFocal_df.columns=SolicitudDeInversiónSocialFocal_df.iloc[0,:]\n",
    "SolicitudDeInversiónSocialFocal_df=SolicitudDeInversiónSocialFocal_df.iloc[1:,:]\n",
    "df=SolicitudDeInversiónSocialFocal_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"RecolectanInformaciónDeLosTitularesDeResponsabilidadesfamiliasComunidadesOrganizacionesDeLaSociedadCivilOSCSindicatosYDeObligacionesinstitucionesPúblicasDelEstadoYLosGobernantes\"]]\n",
    "df.columns=[\"ID\",\"Recolectan\"]\n",
    "df=df.iloc[rn:,:]\n",
    "Actividades_d = Actividades.get_all_values()\n",
    "Actividades_df=pd.DataFrame(Actividades_d)\n",
    "Actividades_df.columns=Actividades_df.iloc[0,:]\n",
    "Actividades_df=Actividades_df.iloc[2:,:]\n",
    "Actividades_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"MediosDeVerificación\"]]\n",
    "Actividades_df[\"MediosDeVerificación\"] = Actividades_df[\"MediosDeVerificación\"].str.replace('\\n', ' ', regex=True)\n",
    "df2=Actividades_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"MediosDeVerificación\"]]\n",
    "df2.columns=[\"ID\",\"MediosDeVerificación\"]\n",
    "df2=df2.iloc[rn2:,:]\n",
    "df_final= pd.merge(df,df2, on=\"ID\", how=\"inner\")\n",
    "# df_sfinal = pd.merge(Puntaje_df,Rubrica_df, on='GeneracionConocimiento', how='inner')\n",
    "# df_final= pd.merge(df_sfinal,df_2, on=\"ID\", how=\"inner\")\n",
    "# df_final = df_final.iloc[:,[0,3,4,2]]\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "df_final['Recolectan'] = df_final['Recolectan'].apply(quitar_acentos)\n",
    "df_final['Recolectan'] = df_final['Recolectan'].astype(str)\n",
    "df_final['MediosDeVerificación'] = df_final['MediosDeVerificación'].apply(quitar_acentos)\n",
    "df_final['MediosDeVerificación'] = df_final['MediosDeVerificación'].astype(str)\n",
    "text = list(df_final['Recolectan'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Recolectan'] = corpus\n",
    "text = list(df_final['MediosDeVerificación'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['MediosDeVerificación'] = corpus\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text1=df_final['Recolectan'] = df_final['Recolectan'].apply(quitar_acentos)\n",
    "X_text2=df_final['MediosDeVerificación'] = df_final['MediosDeVerificación'].apply(quitar_acentos)\n",
    "\n",
    "with open(\"tfidf1_model_td6.pkl\", \"rb\") as f:  \n",
    "    tfidf1_td6_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf2_model_td6.pkl\", \"rb\") as f:  \n",
    "    tfidf2_td6_loaded = pickle.load(f)  \n",
    "\n",
    "with open (\"RFbuenoTD6.pkl\",\"rb\") as modelo_6:\n",
    "    modelo_td6 = pickle.load(modelo_6)\n",
    "\n",
    "\n",
    "X_text1_t=tfidf1_td6_loaded.transform(X_text1)\n",
    "X_text2_t=tfidf2_td6_loaded.transform(X_text2)\n",
    "\n",
    "X_combined = hstack([X_text1_t, X_text2_t])\n",
    "\n",
    "prediccion = modelo_td6.predict(X_combined)\n",
    "df_final[\"Clasificación\"]= prediccion\n",
    "\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[10,11]]\n",
    "Rubrica_df.columns=[\"GeneracionConocimiento\", \"Clasificación\"]\n",
    "Rubrica_df[\"GeneracionConocimiento\"]=Rubrica_df[\"GeneracionConocimiento\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s=pd.merge(df_final,Rubrica_df, on='Clasificación', how='inner')\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.08,\n",
    "    2: 2.17,\n",
    "    3: 3.25,\n",
    "    4: 4.33,\n",
    "    5: 5.42,\n",
    "    6: 6.5\n",
    "}\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "Puntaje_d = Puntaje.get_all_values()\n",
    "Puntaje_df=pd.DataFrame(Puntaje_d)\n",
    "Puntaje_df=Puntaje_df.iloc[2:,[0,13]]\n",
    "Puntaje_df.columns=[\"ID\",\"GeneracionConocimiento\"]\n",
    "pred_s[\"Puntaje\"] = pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[-2,-1]]\n",
    "last_row = len(Puntaje.get_all_values()) + 1 # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"N{last_row-len(pred_s)}\", values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disponibilidad de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
    "rn2=int(RN_df.loc[:,\"Actividades\"])-3\n",
    "print(rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libro = client.open_by_key(\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\") #Original\n",
    "SolicitudDeInversiónSocialFocal=libro.get_worksheet(1)\n",
    "Actividades=libro.get_worksheet(5)\n",
    "SolicitudDeInversiónSocialFocal_d = SolicitudDeInversiónSocialFocal.get_all_values()\n",
    "SolicitudDeInversiónSocialFocal_df=pd.DataFrame(SolicitudDeInversiónSocialFocal_d)\n",
    "SolicitudDeInversiónSocialFocal_df.columns=SolicitudDeInversiónSocialFocal_df.iloc[0,:]\n",
    "SolicitudDeInversiónSocialFocal_df=SolicitudDeInversiónSocialFocal_df.iloc[1:,:]\n",
    "df=SolicitudDeInversiónSocialFocal_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"CómoSeleccionanALosSujetosDeDerecho\",\"QuéTipoDeInformaciónLesSolicitanALasPersonasAtendidasPorPrimeraVez\",\"CómoLlevaACaboElSeguimientoDeLasDistintasAccionesYResultadosGeneradosEnElMarcoDeSusIntervenciones2\",\"IndicadoresProposito\",\"RecolectanInformaciónDeLosTitularesDeResponsabilidadesfamiliasComunidadesOrganizacionesDeLaSociedadCivilOSCSindicatosYDeObligacionesinstitucionesPúblicasDelEstadoYLosGobernantes\",\"EnCasoDeQueSíquéTipoDeInformaciónLesSolicitanConQuéPeriodicidadCuentanConInstrumentosDeRecolecciónEstandarizadosParaRecolectarEstaInformación\"]]\n",
    "df.columns=[\"ID\",\"Selección\",\"InformaciónSolicitada\",\"Seguimiento\",\"IndicadoresProposito\",\"RecollecionTitulares\",\"InfoPeriodicidad\"]\n",
    "df=df.iloc[rn:,:]\n",
    "Actividades_d = Actividades.get_all_values()\n",
    "Actividades_df=pd.DataFrame(Actividades_d)\n",
    "Actividades_df.columns=Actividades_df.iloc[0,:]\n",
    "Actividades_df=Actividades_df.iloc[2:,:]\n",
    "Actividades_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"MediosDeVerificación\"]]\n",
    "Actividades_df[\"MediosDeVerificación\"] = Actividades_df[\"MediosDeVerificación\"].str.replace('\\n', ' ', regex=True)\n",
    "df2=Actividades_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"MediosDeVerificación\"]]\n",
    "df2.columns=[\"ID\",\"MediosDeVerificación\"]\n",
    "df2=df2.iloc[rn2:,:]\n",
    "df_final= pd.merge(df,df2, on=\"ID\", how=\"inner\")\n",
    "# df_sfinal = pd.merge(Puntaje_df,Rubrica_df, on='DisponibilidadInfo', how='inner')\n",
    "# df_final= pd.merge(df_sfinal,df_2, on=\"ID\", how=\"inner\")\n",
    "# df_final = df_final.iloc[:,[0,3,4,5,6,7,8,9,2]]\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "df_final['Selección'] = df_final['Selección'].apply(quitar_acentos)\n",
    "df_final['Selección'] = df_final['Selección'].astype(str)\n",
    "df_final['InformaciónSolicitada'] = df_final['InformaciónSolicitada'].apply(quitar_acentos)\n",
    "df_final['InformaciónSolicitada'] = df_final['InformaciónSolicitada'].astype(str)\n",
    "df_final['Seguimiento'] = df_final['Seguimiento'].apply(quitar_acentos)\n",
    "df_final['Seguimiento'] = df_final['Seguimiento'].astype(str)\n",
    "df_final['IndicadoresProposito'] = df_final['IndicadoresProposito'].apply(quitar_acentos)\n",
    "df_final['IndicadoresProposito'] = df_final['IndicadoresProposito'].astype(str)\n",
    "df_final['RecollecionTitulares'] = df_final['RecollecionTitulares'].apply(quitar_acentos)\n",
    "df_final['RecollecionTitulares'] = df_final['RecollecionTitulares'].astype(str)\n",
    "df_final['InfoPeriodicidad'] = df_final['InfoPeriodicidad'].apply(quitar_acentos)\n",
    "df_final['InfoPeriodicidad'] = df_final['InfoPeriodicidad'].astype(str)\n",
    "df_final['MediosDeVerificación'] = df_final['MediosDeVerificación'].apply(quitar_acentos)\n",
    "df_final['MediosDeVerificación'] = df_final['MediosDeVerificación'].astype(str)\n",
    "text = list(df_final['Selección'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Selección'] = corpus\n",
    "text = list(df_final['InformaciónSolicitada'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['InformaciónSolicitada'] = corpus\n",
    "text = list(df_final['Seguimiento'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Seguimiento'] = corpus\n",
    "text = list(df_final['IndicadoresProposito'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['IndicadoresProposito'] = corpus\n",
    "text = list(df_final['RecollecionTitulares'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['RecollecionTitulares'] = corpus\n",
    "text = list(df_final['InfoPeriodicidad'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['InfoPeriodicidad'] = corpus\n",
    "text = list(df_final['MediosDeVerificación'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['MediosDeVerificación'] = corpus\n",
    "\n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf1_model_td7.pkl\", \"rb\") as f:  \n",
    "    tfidf1_td7_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf2_model_td7.pkl\", \"rb\") as f:  \n",
    "    tfidf2_td7_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf3_model_td7.pkl\", \"rb\") as f:  \n",
    "    tfidf3_td7_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf4_model_td7.pkl\", \"rb\") as f:  \n",
    "    tfidf4_td7_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf5_model_td7.pkl\", \"rb\") as f:  \n",
    "    tfidf5_td7_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf6_model_td7.pkl\", \"rb\") as f:  \n",
    "    tfidf6_td7_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf7_model_td7.pkl\", \"rb\") as f:  \n",
    "    tfidf7_td7_loaded = pickle.load(f)  \n",
    "\n",
    "with open (\"RFbuenoTD7.pkl\",\"rb\") as modelo_7:\n",
    "    modelo_td7 = pickle.load(modelo_7)\n",
    "\n",
    "\n",
    "\n",
    "X_text1=df_final['Selección'] = df_final['Selección'].apply(quitar_acentos)\n",
    "X_text2=df_final['InformaciónSolicitada'] = df_final['InformaciónSolicitada'].apply(quitar_acentos)\n",
    "X_text3=df_final['Seguimiento'] = df_final['Seguimiento'].apply(quitar_acentos)\n",
    "X_text4=df_final['IndicadoresProposito'] = df_final['IndicadoresProposito'].apply(quitar_acentos)\n",
    "X_text5=df_final['RecollecionTitulares'] = df_final['RecollecionTitulares'].apply(quitar_acentos)\n",
    "X_text6=df_final['InfoPeriodicidad'] = df_final['InfoPeriodicidad'].apply(quitar_acentos)\n",
    "X_text7=df_final['MediosDeVerificación'] = df_final['MediosDeVerificación'].apply(quitar_acentos)\n",
    "\n",
    "X_text1_t=tfidf1_td7_loaded.transform(X_text1)\n",
    "X_text2_t=tfidf2_td7_loaded.transform(X_text2)\n",
    "X_text3_t=tfidf3_td7_loaded.transform(X_text3)\n",
    "X_text4_t=tfidf4_td7_loaded.transform(X_text4)\n",
    "X_text5_t=tfidf5_td7_loaded.transform(X_text5)\n",
    "X_text6_t=tfidf6_td7_loaded.transform(X_text6)\n",
    "X_text7_t=tfidf7_td7_loaded.transform(X_text7)\n",
    "\n",
    "\n",
    "X_combined = hstack([X_text1_t, X_text2_t,X_text3_t, X_text4_t,X_text5_t, X_text6_t,X_text7_t])\n",
    "\n",
    "prediccion = modelo_td7.predict(X_combined)\n",
    "df_final[\"Clasificación\"]=prediccion\n",
    "df_final\n",
    "\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[12,13]]\n",
    "Rubrica_df.columns=[\"DisponibilidadInfo\", \"Clasificación\"]\n",
    "Rubrica_df[\"DisponibilidadInfo\"]=Rubrica_df[\"DisponibilidadInfo\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s = pd.merge(df_final,Rubrica_df, on=\"Clasificación\",how=\"inner\")\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.08,\n",
    "    2: 2.17,\n",
    "    3: 3.25,\n",
    "    4: 4.33,\n",
    "    5: 5.42,\n",
    "    6: 6.5\n",
    "}\n",
    "\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "Puntaje_d = Puntaje.get_all_values()\n",
    "Puntaje_df=pd.DataFrame(Puntaje_d)\n",
    "Puntaje_df=Puntaje_df.iloc[2:,[0,15]]\n",
    "Puntaje_df.columns=[\"ID\",\"DisponibilidadInfo\"]\n",
    "pred_s[\"Puntaje\"]=pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[-2,-1]]\n",
    "last_row = len(Puntaje.get_all_values()) + 1 # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"P{last_row-len(pred_s)}\", values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso del conocimiento para la toma de decisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
    "print(rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libro = client.open_by_key(\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\")\n",
    "SolicitudDeInversiónSocialFocal=libro.get_worksheet(1)\n",
    "SolicitudDeInversiónSocialFocal_d = SolicitudDeInversiónSocialFocal.get_all_values()\n",
    "SolicitudDeInversiónSocialFocal_df=pd.DataFrame(SolicitudDeInversiónSocialFocal_d)\n",
    "SolicitudDeInversiónSocialFocal_df.columns = SolicitudDeInversiónSocialFocal_df.iloc[0,:]\n",
    "SolicitudDeInversiónSocialFocal_df = SolicitudDeInversiónSocialFocal_df.iloc[1:]\n",
    "df=SolicitudDeInversiónSocialFocal_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"ConsideraRealizarUnaSiguienteEtapaDeSuProyecto\"]]\n",
    "df.columns=[\"ID\", \"Respuesta\"]\n",
    "df_final=df.iloc[rn:,:]\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "\n",
    "# df_sfinal = pd.merge(Puntaje_df,Rubrica_df, on='ConocimientoDecisiones', how='inner')\n",
    "# df_final= pd.merge(df_sfinal,df, on=\"ID\", how=\"inner\")\n",
    "# df_final=df_final.iloc[:,[3,2]]\n",
    "df_final['Respuesta'] = df_final['Respuesta'].apply(quitar_acentos)\n",
    "df_final['Respuesta'] = df_final['Respuesta'].astype(str)\n",
    "# df_final['Clasificación'] = df_final['Clasificación'].replace(1, 0)\n",
    "text = list(df_final['Respuesta'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Respuesta'] = corpus\n",
    "X = df_final['Respuesta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"RFbuenoTD8.pkl\",\"rb\") as modelo_8:\n",
    "    modelo_td8 = pickle.load(modelo_8)\n",
    "\n",
    "prediccion = modelo_td8.predict(X)\n",
    "df_final[\"Clasificación\"]= prediccion\n",
    "\n",
    "\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[14,15]]\n",
    "Rubrica_df.columns=[\"ConocimientoDecisiones\", \"Clasificación\"]\n",
    "Rubrica_df[\"ConocimientoDecisiones\"]=Rubrica_df[\"ConocimientoDecisiones\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s=pd.merge(df_final,Rubrica_df, on='Clasificación', how='inner')\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.17,\n",
    "    2: 2.33,\n",
    "    3: 3.50,\n",
    "    4: 4.67,\n",
    "    5: 5.83,\n",
    "    6: 7\n",
    "}\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "Puntaje_d = Puntaje.get_all_values()\n",
    "Puntaje_df=pd.DataFrame(Puntaje_d)\n",
    "Puntaje_df=Puntaje_df.iloc[2:,[0,17]]\n",
    "Puntaje_df.columns=[\"ID\",\"ConocimientoDecisiones\"]\n",
    "pred_s[\"Puntaje\"] = pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[-2,-1]]\n",
    "last_row = len(Puntaje.get_all_values()) + 1 # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"R{last_row-len(pred_s)}\", values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos para resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
    "print(rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libro = client.open_by_key(\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\") #Original\n",
    "SolicitudDeInversiónSocialFocal=libro.get_worksheet(1)\n",
    "SolicitudDeInversiónSocialFocal_d = SolicitudDeInversiónSocialFocal.get_all_values()\n",
    "SolicitudDeInversiónSocialFocal_df=pd.DataFrame(SolicitudDeInversiónSocialFocal_d)\n",
    "SolicitudDeInversiónSocialFocal_df.columns=SolicitudDeInversiónSocialFocal_df.iloc[0,:]\n",
    "SolicitudDeInversiónSocialFocal_df=SolicitudDeInversiónSocialFocal_df.iloc[1:,:]\n",
    "df=SolicitudDeInversiónSocialFocal_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"EnlistaLosGastosEnRecursosHumanos\",\"DescripciónDeGastosEnRecursosHumanos\",\"DescripciónDeGastosEnInsumosMateriales\",\"DescripciónDeGastosEnServicios\",\"EnlistaLosGastosParaInmueble\",\"DescripciónDeGastosParaInmuebles\",\"MontoDeGastosParaTrasladosYViáticos\",\"EnlistaLosGastosEnTrasladosYViáticos\",\"DescripciónDeGastosEnTrasladosYViáticos\",\"DescripciónDeGastosEnMobiliarioYEquipo\",\"TipoSolicitud\"]]\n",
    "df.columns=[\"ID\",\"GastosRh\",\"DescripcionGastosRh\",\"DescripcionGastosInsumos\",\"DescripcionGastosServicios\",\"GastosInmueble\",\"DescripcionGastosInmueble\",\"MontoViaticos\",\"GastosViaticos\",\"DescripcionGastosViaticos\",\"DescripcionGastosEquipo\",\"TipoSolicitud\"]\n",
    "df_final=df.iloc[rn:,:]\n",
    "\n",
    "\n",
    "# df_sfinal = pd.merge(Puntaje_df,Rubrica_df, on='RecursosResultados', how='inner')\n",
    "# df_final= pd.merge(df_sfinal,df, on=\"ID\", how=\"inner\")\n",
    "# df_final = df_final.iloc[:,[0,3,4,5,6,7,8,9,10,11,12,13,2]]\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "df_final['GastosRh'] = df_final['GastosRh'].apply(quitar_acentos)\n",
    "df_final['GastosRh'] = df_final['GastosRh'].astype(str)\n",
    "df_final['DescripcionGastosRh'] = df_final['DescripcionGastosRh'].apply(quitar_acentos)\n",
    "df_final['DescripcionGastosRh'] = df_final['DescripcionGastosRh'].astype(str)\n",
    "df_final['DescripcionGastosInsumos'] = df_final['DescripcionGastosInsumos'].apply(quitar_acentos)\n",
    "df_final['DescripcionGastosInsumos'] = df_final['DescripcionGastosInsumos'].astype(str)\n",
    "df_final['DescripcionGastosServicios'] = df_final['DescripcionGastosServicios'].apply(quitar_acentos)\n",
    "df_final['DescripcionGastosServicios'] = df_final['DescripcionGastosServicios'].astype(str)\n",
    "df_final['GastosInmueble'] = df_final['GastosInmueble'].apply(quitar_acentos)\n",
    "df_final['GastosInmueble'] = df_final['GastosInmueble'].astype(str)\n",
    "df_final['DescripcionGastosInmueble'] = df_final['DescripcionGastosInmueble'].apply(quitar_acentos)\n",
    "df_final['DescripcionGastosInmueble'] = df_final['DescripcionGastosInmueble'].astype(str)\n",
    "df_final['GastosViaticos'] = df_final['GastosViaticos'].apply(quitar_acentos)\n",
    "df_final['GastosViaticos'] = df_final['GastosViaticos'].astype(str)\n",
    "df_final['DescripcionGastosViaticos'] = df_final['DescripcionGastosViaticos'].apply(quitar_acentos)\n",
    "df_final['DescripcionGastosViaticos'] = df_final['DescripcionGastosViaticos'].astype(str)\n",
    "df_final['DescripcionGastosEquipo'] = df_final['DescripcionGastosEquipo'].apply(quitar_acentos)\n",
    "df_final['DescripcionGastosEquipo'] = df_final['DescripcionGastosEquipo'].astype(str)\n",
    "df_final['TipoSolicitud'] = df_final['TipoSolicitud'].apply(quitar_acentos)\n",
    "df_final['TipoSolicitud'] = df_final['TipoSolicitud'].astype(str)\n",
    "\n",
    "text = list(df_final['GastosRh'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['GastosRh'] = corpus\n",
    "text = list(df_final['DescripcionGastosRh'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['DescripcionGastosRh'] = corpus\n",
    "text = list(df_final['DescripcionGastosInsumos'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['DescripcionGastosInsumos'] = corpus\n",
    "text = list(df_final['DescripcionGastosServicios'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['DescripcionGastosServicios'] = corpus\n",
    "text = list(df_final['GastosInmueble'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['GastosInmueble'] = corpus\n",
    "text = list(df_final['DescripcionGastosInmueble'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['DescripcionGastosInmueble'] = corpus\n",
    "text = list(df_final['GastosViaticos'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['GastosViaticos'] = corpus\n",
    "text = list(df_final['DescripcionGastosViaticos'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['DescripcionGastosViaticos'] = corpus\n",
    "text = list(df_final['DescripcionGastosEquipo'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['DescripcionGastosEquipo'] = corpus\n",
    "text = list(df_final['TipoSolicitud'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['TipoSolicitud'] = corpus\n",
    "df_final['MontoViaticos'] = df_final['MontoViaticos'].astype(str)\n",
    "df_final['MontoViaticos'] = df_final['MontoViaticos'].str.replace(r'\\D', '', regex=True)\n",
    "df_final['MontoViaticos'] = pd.to_numeric(df_final['MontoViaticos'], errors='coerce').fillna(0).astype(int)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf1_model_td9.pkl\", \"rb\") as f:  \n",
    "    tfidf1_td9_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf2_model_td9.pkl\", \"rb\") as f:  \n",
    "    tfidf2_td9_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf3_model_td9.pkl\", \"rb\") as f:  \n",
    "    tfidf3_td9_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf4_model_td9.pkl\", \"rb\") as f:  \n",
    "    tfidf4_td9_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf5_model_td9.pkl\", \"rb\") as f:  \n",
    "    tfidf5_td9_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf6_model_td9.pkl\", \"rb\") as f:  \n",
    "    tfidf6_td9_loaded = pickle.load(f)\n",
    "\n",
    "with open(\"scaler_model_td9.pkl\", \"rb\") as f:  \n",
    "    scaler_td9_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf8_model_td9.pkl\", \"rb\") as f:  \n",
    "    tfidf8_td9_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf9_model_td9.pkl\", \"rb\") as f:  \n",
    "    tfidf9_td9_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf10_model_td9.pkl\", \"rb\") as f:  \n",
    "    tfidf10_td9_loaded = pickle.load(f)  \n",
    "\n",
    "with open (\"RFbuenoTD9.pkl\",\"rb\") as modelo_9:\n",
    "    modelo_td9 = pickle.load(modelo_9)\n",
    "\n",
    "X_text1=df_final['GastosRh'] = df_final['GastosRh'].apply(quitar_acentos)\n",
    "X_text2=df_final['DescripcionGastosRh'] = df_final['DescripcionGastosRh'].apply(quitar_acentos)\n",
    "X_text3=df_final['DescripcionGastosInsumos'] = df_final['DescripcionGastosInsumos'].apply(quitar_acentos)\n",
    "X_text4=df_final['DescripcionGastosServicios'] = df_final['DescripcionGastosServicios'].apply(quitar_acentos)\n",
    "X_text5=df_final['GastosInmueble'] = df_final['GastosInmueble'].apply(quitar_acentos)\n",
    "X_text6=df_final['DescripcionGastosInmueble'] = df_final['DescripcionGastosInmueble'].apply(quitar_acentos)\n",
    "x_num = df_final[['MontoViaticos']]\n",
    "X_text8=df_final['DescripcionGastosEquipo'] = df_final['DescripcionGastosEquipo'].apply(quitar_acentos)\n",
    "X_text9=df_final['DescripcionGastosViaticos'] = df_final['DescripcionGastosViaticos'].apply(quitar_acentos)\n",
    "X_text10=df_final['TipoSolicitud'] = df_final['TipoSolicitud'].apply(quitar_acentos)\n",
    "\n",
    "\n",
    "X_text1_t=tfidf1_td9_loaded.transform(X_text1)\n",
    "X_text2_t=tfidf2_td9_loaded.transform(X_text2)\n",
    "X_text3_t=tfidf3_td9_loaded.transform(X_text3)\n",
    "X_text4_t=tfidf4_td9_loaded.transform(X_text4)\n",
    "X_text5_t=tfidf5_td9_loaded.transform(X_text5)\n",
    "X_text6_t=tfidf6_td9_loaded.transform(X_text6)\n",
    "x_num_t=scaler_td9_loaded.transform(x_num)\n",
    "X_text8_t=tfidf8_td9_loaded.transform(X_text8)\n",
    "X_text9_t=tfidf9_td9_loaded.transform(X_text9)\n",
    "X_text10_t=tfidf10_td9_loaded.transform(X_text10)\n",
    "\n",
    "X_combined = hstack([X_text1_t, X_text2_t, X_text3_t, X_text4_t, X_text5_t, X_text6_t, X_text8_t, X_text9_t, X_text10_t,x_num_t])\n",
    "\n",
    "prediccion = modelo_td9.predict(X_combined)\n",
    "df_final[\"Clasificación\"]= prediccion\n",
    "df_final\n",
    "\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[16,17]]\n",
    "Rubrica_df.columns=[\"RecursosResultados\", \"Clasificación\"]\n",
    "Rubrica_df[\"RecursosResultados\"]=Rubrica_df[\"RecursosResultados\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s=pd.merge(df_final,Rubrica_df, on='Clasificación', how='inner')\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.67,\n",
    "    2: 3.33,\n",
    "    3: 5.00,\n",
    "    4: 6.67,\n",
    "    5: 8.33,\n",
    "    6: 10\n",
    "}\n",
    "pred_s[\"Puntaje\"] = pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[-2,-1]]\n",
    "pred_s\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "last_row = len(Puntaje.get_all_values()) + 1 # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"U{last_row-len(pred_s)}\", values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planeación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
    "print(rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libro = client.open_by_key(\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\") #Original\n",
    "SolicitudDeInversiónSocialFocal=libro.get_worksheet(1)\n",
    "SolicitudDeInversiónSocialFocal_d = SolicitudDeInversiónSocialFocal.get_all_values()\n",
    "SolicitudDeInversiónSocialFocal_df=pd.DataFrame(SolicitudDeInversiónSocialFocal_d)\n",
    "SolicitudDeInversiónSocialFocal_df.columns=SolicitudDeInversiónSocialFocal_df.iloc[0,:]\n",
    "SolicitudDeInversiónSocialFocal_df=SolicitudDeInversiónSocialFocal_df.iloc[1:,:]\n",
    "df=SolicitudDeInversiónSocialFocal_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"FundamentaDeManeraBreveCómoEsQueElConjuntoDeProductosServiciosYCualquierOtroComponenteDeSusIntervencionesFavoreceráElCambioEsperado\",\"SusActividadesImplicanElInvolucramientoDeLosTitularesDeResponsabilidadesfamiliasComunidadesOrganizacionesDeLaSociedadCivilOSCSindicatosYDeObligacionesinstitucionesPúblicasDelEstadoYLosGobernantesQuéSeRealizaConEllosasQuéBuscaGenerarEnLosMismos\"]]\n",
    "df.columns=[\"ID\",\"IntervencionBeneficio\",\"InvolucramientoTitulares\"]\n",
    "df_final=df.iloc[rn:,:]\n",
    "df_final\n",
    "# df_sfinal = pd.merge(Puntaje_df,Rubrica_df, on='Planeacion', how='inner')\n",
    "# df_final= pd.merge(df_sfinal,df, on=\"ID\", how=\"inner\")\n",
    "# df_final = df_final.iloc[:,[0,3,4,2]]\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "df_final['IntervencionBeneficio'] = df_final['IntervencionBeneficio'].apply(quitar_acentos)\n",
    "df_final['IntervencionBeneficio'] = df_final['IntervencionBeneficio'].astype(str)\n",
    "df_final['InvolucramientoTitulares'] = df_final['InvolucramientoTitulares'].apply(quitar_acentos)\n",
    "df_final['InvolucramientoTitulares'] = df_final['InvolucramientoTitulares'].astype(str)\n",
    "text = list(df_final['IntervencionBeneficio'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['IntervencionBeneficio'] = corpus\n",
    "text = list(df_final['InvolucramientoTitulares'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['InvolucramientoTitulares'] = corpus\n",
    "\n",
    "X_text1=df_final['IntervencionBeneficio'] = df_final['IntervencionBeneficio'].apply(quitar_acentos)\n",
    "X_text2=df_final['InvolucramientoTitulares'] = df_final['InvolucramientoTitulares'].apply(quitar_acentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf1_model_td10.pkl\", \"rb\") as f:  \n",
    "    tfidf1_td10_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf2_model_td10.pkl\", \"rb\") as f:  \n",
    "    tfidf2_td10_loaded = pickle.load(f)  \n",
    "\n",
    "with open (\"MNB_buenoTD10.pkl\",\"rb\") as modelo_10:\n",
    "    modelo_td10 = pickle.load(modelo_10)\n",
    "\n",
    "X_text1_t=tfidf1_td10_loaded.transform(X_text1)\n",
    "X_text2_t=tfidf2_td10_loaded.transform(X_text2)\n",
    "\n",
    "X_combined = hstack([X_text1_t, X_text2_t])\n",
    "\n",
    "prediccion = modelo_td10.predict(X_combined)\n",
    "df_final[\"Clasificación\"]= prediccion\n",
    "\n",
    "\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[18,19]]\n",
    "Rubrica_df.columns=[\"Planeacion\", \"Clasificación\"]\n",
    "Rubrica_df[\"Planeacion\"]=Rubrica_df[\"Planeacion\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s=pd.merge(df_final,Rubrica_df, on='Clasificación', how='inner')\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.67,\n",
    "    2: 3.33,\n",
    "    3: 5.00,\n",
    "    4: 6.67,\n",
    "    5: 8.33,\n",
    "    6: 10\n",
    "}\n",
    "\n",
    "\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "Puntaje_d = Puntaje.get_all_values()\n",
    "Puntaje_df=pd.DataFrame(Puntaje_d)\n",
    "Puntaje_df=Puntaje_df.iloc[2:,[0,22]]\n",
    "Puntaje_df.columns=[\"ID\",\"Planeacion\"]\n",
    "pred_s[\"Puntaje\"] = pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[-2,-1]]\n",
    "\n",
    "last_row = len(Puntaje.get_all_values()) + 1 # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"W{last_row-len(pred_s)}\", values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alianzas de complementariedad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn=int(RN_df.loc[:,\"CoinversoresTabla\"])-3\n",
    "rn2=int(RN_df.loc[:,\"AliadosDelProyecto\"])-3\n",
    "\n",
    "print(rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libro = client.open_by_key(\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\") #Original\n",
    "CoinversoresTabla=libro.get_worksheet(11)\n",
    "AliadosDelProyecto=libro.get_worksheet(2)\n",
    "CoinversoresTabla_d = CoinversoresTabla.get_all_values()\n",
    "CoinversoresTabla_df=pd.DataFrame(CoinversoresTabla_d)\n",
    "CoinversoresTabla_df.columns=CoinversoresTabla_df.iloc[0,:]\n",
    "CoinversoresTabla_df=CoinversoresTabla_df.iloc[2:,:]\n",
    "df=CoinversoresTabla_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"Coinversor\",\"MontoCoinversor\"]]\n",
    "df.columns=[\"ID\",\"Coinversor\",\"MontoCoinversor\"]\n",
    "df=df.iloc[rn:,:]\n",
    "AliadosDelProyecto=libro.get_worksheet(2)\n",
    "AliadosDelProyecto_d = AliadosDelProyecto.get_all_values()\n",
    "AliadosDelProyecto_df=pd.DataFrame(AliadosDelProyecto_d)\n",
    "AliadosDelProyecto_df.columns=AliadosDelProyecto_df.iloc[0,:]\n",
    "AliadosDelProyecto_df=AliadosDelProyecto_df.iloc[2:,:]\n",
    "df2=AliadosDelProyecto_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"NombreAliado\",\"SectorAliado\",\"QuéPapelTieneElAliadoEnElProyecto\"]]\n",
    "df2.columns=[\"ID\",\"NombreAliado\",\"SectorAliado\",\"QuéPapelTieneElAliadoEnElProyecto\"]\n",
    "df2=df2.iloc[rn2:,:]\n",
    "df_final=pd.merge(df,df2, on=\"ID\", how=\"inner\")\n",
    "\n",
    "# df_sfinal = pd.merge(Puntaje_df,Rubrica_df, on='Alianzas', how='inner')\n",
    "# df_final= pd.merge(df_sfinal,df3, on=\"ID\", how=\"inner\")\n",
    "# df_final = df_final.iloc[:,[0,3,4,5,6,7,2]]\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "df_final['Coinversor'] = df_final['Coinversor'].apply(quitar_acentos)\n",
    "df_final['Coinversor'] = df_final['Coinversor'].astype(str)\n",
    "df_final['NombreAliado'] = df_final['NombreAliado'].apply(quitar_acentos)\n",
    "df_final['NombreAliado'] = df_final['NombreAliado'].astype(str)\n",
    "df_final['SectorAliado'] = df_final['SectorAliado'].apply(quitar_acentos)\n",
    "df_final['SectorAliado'] = df_final['SectorAliado'].astype(str)\n",
    "df_final['QuéPapelTieneElAliadoEnElProyecto'] = df_final['QuéPapelTieneElAliadoEnElProyecto'].apply(quitar_acentos)\n",
    "df_final['QuéPapelTieneElAliadoEnElProyecto'] = df_final['QuéPapelTieneElAliadoEnElProyecto'].astype(str)\n",
    "text = list(df_final['Coinversor'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['Coinversor'] = corpus\n",
    "text = list(df_final['NombreAliado'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['NombreAliado'] = corpus\n",
    "text = list(df_final['SectorAliado'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['SectorAliado'] = corpus\n",
    "text = list(df_final['QuéPapelTieneElAliadoEnElProyecto'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['QuéPapelTieneElAliadoEnElProyecto'] = corpus\n",
    "df_final['MontoCoinversor'] = df_final['MontoCoinversor'].astype(str) \n",
    "df_final['MontoCoinversor'] = df_final['MontoCoinversor'].str.replace(r'\\D', '', regex=True)\n",
    "df_final['MontoCoinversor'] = pd.to_numeric(df_final['MontoCoinversor'], errors='coerce').fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf1_model_td11.pkl\", \"rb\") as f:  \n",
    "    tfidf1_td11_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf2_model_td11.pkl\", \"rb\") as f:  \n",
    "    tfidf2_td11_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf3_model_td11.pkl\", \"rb\") as f:  \n",
    "    tfidf3_td11_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"tfidf4_model_td11.pkl\", \"rb\") as f:  \n",
    "    tfidf4_td11_loaded = pickle.load(f) \n",
    "\n",
    "with open(\"scaler_model_td11.pkl\", \"rb\") as f:  \n",
    "    scaler_td11_loaded = pickle.load(f)  \n",
    "\n",
    "X_text1 = df_final['Coinversor'].apply(quitar_acentos)\n",
    "X_text2 = df_final['NombreAliado'].apply(quitar_acentos)\n",
    "X_text3 = df_final['SectorAliado'].apply(quitar_acentos)\n",
    "X_text4 = df_final['QuéPapelTieneElAliadoEnElProyecto'].apply(quitar_acentos)\n",
    "\n",
    "df_final['MontoCoinversor'] = df_final['MontoCoinversor'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "df_final['MontoCoinversor'] = pd.to_numeric(df_final['MontoCoinversor'], errors='coerce').fillna(0).astype(int)\n",
    "X_num = df_final[['MontoCoinversor']] \n",
    "\n",
    "X_text1_t=tfidf1_td11_loaded.transform(X_text1)\n",
    "X_text2_t=tfidf2_td11_loaded.transform(X_text2)\n",
    "X_text3_t=tfidf3_td11_loaded.transform(X_text3)\n",
    "X_text4_t=tfidf4_td11_loaded.transform(X_text4)\n",
    "x_num_t=scaler_td11_loaded.transform(X_num)\n",
    "\n",
    "with open (\"RF_buenoTD11.pkl\",\"rb\") as modelo_11:\n",
    "    modelo_td11 = pickle.load(modelo_11)\n",
    "\n",
    "\n",
    "X_combined=hstack([X_text1_t, X_text2_t, X_text3_t, X_text4_t, x_num_t])\n",
    "\n",
    "prediccion = modelo_td11.predict(X_combined)\n",
    "df_final[\"Clasificación\"]= prediccion\n",
    "\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[20,21]]\n",
    "Rubrica_df.columns=[\"Alianzas\", \"Clasificación\"]\n",
    "Rubrica_df[\"Alianzas\"]=Rubrica_df[\"Alianzas\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s=pd.merge(df_final,Rubrica_df, on='Clasificación', how='inner')\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.67,\n",
    "    2: 3.33,\n",
    "    3: 5.00,\n",
    "    4: 6.67,\n",
    "    5: 8.33,\n",
    "    6: 10\n",
    "}\n",
    "\n",
    "\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "Puntaje_d = Puntaje.get_all_values()\n",
    "Puntaje_df=pd.DataFrame(Puntaje_d)\n",
    "Puntaje_df=Puntaje_df.iloc[2:,[0,25]]\n",
    "Puntaje_df.columns=[\"ID\",\"Alianzas\"]\n",
    "pred_s[\"Puntaje\"] = pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[-2,-1]]\n",
    "last_row = len(Puntaje.get_all_values()) + 1 # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"Z{last_row-len(pred_s)}\", values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id=\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\"\n",
    "\n",
    "libro = client.open_by_key(sheet_id) #Original\n",
    "\n",
    "RN=libro.get_worksheet(12)\n",
    "RN_d = RN.get_all_values()\n",
    "RN_df=pd.DataFrame(RN_d)\n",
    "RN_df.columns=RN_df.iloc[0,:]\n",
    "RN_df=RN_df.iloc[1:,:]\n",
    "rn=int(RN_df.loc[:,\"SolicitudDeInversiónSocialFocal\"])-2\n",
    "rn2=int(RN_df.loc[:,\"DesgloseDeSujetosDeDerecho\"])-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libro = client.open_by_key(\"1r6mOjvi5ITuIgVJn9ec6hNph_ECWywA90uJxZnNYYCY\") #Original\n",
    "SolicitudDeInversiónSocialFocal=libro.get_worksheet(1)\n",
    "SolicitudDeInversiónSocialFocal=libro.get_worksheet(1)\n",
    "SolicitudDeInversiónSocialFocal_d = SolicitudDeInversiónSocialFocal.get_all_values()\n",
    "SolicitudDeInversiónSocialFocal_df=pd.DataFrame(SolicitudDeInversiónSocialFocal_d)\n",
    "SolicitudDeInversiónSocialFocal_df.columns=SolicitudDeInversiónSocialFocal_df.iloc[0,:]\n",
    "SolicitudDeInversiónSocialFocal_df=SolicitudDeInversiónSocialFocal_df.iloc[1:,:]\n",
    "df=SolicitudDeInversiónSocialFocal_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"DeQuéManeraSeInvolucranLosSujetosDeDerechoEnElDesarrolloDeLasActividades\",\"SusActividadesImplicanElInvolucramientoDeLosTitularesDeResponsabilidadesfamiliasComunidadesOrganizacionesDeLaSociedadCivilOSCSindicatosYDeObligacionesinstitucionesPúblicasDelEstadoYLosGobernantesQuéSeRealizaConEllosasQuéBuscaGenerarEnLosMismos\"]]\n",
    "df.columns=[\"ID\",\"InvolucranSujetosDerecho\",\"InvolucramientoTitulares\"]\n",
    "df=df.iloc[rn:,:]\n",
    "\n",
    "Desglose=libro.get_worksheet(4)\n",
    "Desglose_d = Desglose.get_all_values()\n",
    "Desglose_df=pd.DataFrame(Desglose_d)\n",
    "Desglose_df.columns=Desglose_df.iloc[0,:]\n",
    "Desglose_df=Desglose_df.iloc[2:,:]\n",
    "Desglose_df=Desglose_df.loc[:,[\"SolicitudDeInversiónSocialFocal_Id\",\"Edad\",\"NoHombres\",\"NoMujeres\"]]\n",
    "Desglose_df.columns=[\"ID\",\"Edad\",\"Hombres\",\"Mujeres\"]\n",
    "Desglose_df=Desglose_df.iloc[rn2:,:]\n",
    "Desglose_df\n",
    "\n",
    "df_final=pd.merge(df,Desglose_df, on=\"ID\", how=\"inner\")\n",
    "\n",
    "# df_sfinal = pd.merge(Puntaje_df,Rubrica_df, on='Participacion', how='inner')\n",
    "# df_final= pd.merge(df_sfinal,df3, on=\"ID\", how=\"inner\")\n",
    "# df_final = df_final.iloc[:,[0,3,4,5,6,7,2]]\n",
    "acentos = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u',\n",
    "           'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U', \"Ñ\":\"N\", \"ñ\":\"n\"}\n",
    "\n",
    "def quitar_acentos(texto):\n",
    "    for acentuada, sin_acento in acentos.items():\n",
    "        texto = texto.replace(acentuada, sin_acento)\n",
    "    return texto\n",
    "df_final['InvolucranSujetosDerecho'] = df_final['InvolucranSujetosDerecho'].apply(quitar_acentos)\n",
    "df_final['InvolucranSujetosDerecho'] = df_final['InvolucranSujetosDerecho'].astype(str)\n",
    "df_final['InvolucramientoTitulares'] = df_final['InvolucramientoTitulares'].apply(quitar_acentos)\n",
    "df_final['InvolucramientoTitulares'] = df_final['InvolucramientoTitulares'].astype(str)\n",
    "text = list(df_final['InvolucranSujetosDerecho'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['InvolucranSujetosDerecho'] = corpus\n",
    "text = list(df_final['InvolucramientoTitulares'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('spanish')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "df_final['InvolucramientoTitulares'] = corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf_model_td12.pkl\", \"rb\") as f:  \n",
    "    tfidf_td12_loaded = pickle.load(f)  \n",
    "\n",
    "with open(\"scaler_model_td12.pkl\", \"rb\") as f:  \n",
    "    scaler_td12_loaded = pickle.load(f)\n",
    "\n",
    "with open (\"RF_buenoTD12.pkl\",\"rb\") as modelo_12:\n",
    "    modelo_td12 = pickle.load(modelo_12)\n",
    "\n",
    "\n",
    "text_features = ['InvolucranSujetosDerecho', \"InvolucramientoTitulares\"]\n",
    "numeric_features = ['Edad', 'Hombres', 'Mujeres']\n",
    "\n",
    "X_text_t=tfidf_td12_loaded.transform(df_final[text_features].apply(lambda x: ' '.join(x.astype(str)), axis=1))\n",
    "X_numeric_t=scaler_td12_loaded.transform(df_final[numeric_features])\n",
    "\n",
    "X_combined = hstack([X_text_t, X_numeric_t])\n",
    "\n",
    "prediccion = modelo_td12.predict(X_combined)\n",
    "df_final[\"Clasificación\"]= prediccion\n",
    "\n",
    "Cálculo_Puntaje=client.open_by_key(\"1mqxPzWLSjj13hhM4VVJRTWGMsNI1R4-tHgFAbO81MJg\") #Original\n",
    "Rubrica=Cálculo_Puntaje.get_worksheet(1)\n",
    "Rubrica_d = Rubrica.get_all_values()\n",
    "Rubrica_df=pd.DataFrame(Rubrica_d)\n",
    "Rubrica_df=Rubrica_df.iloc[1:,[22,23]]\n",
    "Rubrica_df.columns=[\"Participacion\", \"Clasificación\"]\n",
    "Rubrica_df[\"Participacion\"]=Rubrica_df[\"Participacion\"].astype(str)\n",
    "Rubrica_df['Clasificación'] = range(0, len(Rubrica_df))\n",
    "pred_s=pd.merge(df_final,Rubrica_df, on='Clasificación', how='inner')\n",
    "mapeo = {\n",
    "    0: 0.00,\n",
    "    1: 1.67,\n",
    "    2: 3.33,\n",
    "    3: 5.00,\n",
    "    4: 6.67,\n",
    "    5: 8.33,\n",
    "    6: 10\n",
    "}\n",
    "\n",
    "Puntajes=client.open_by_key(\"1h_RMQImx4eMdcOPX_4oS9Umac168xA45hxXbo3uaX2Y\")\n",
    "Puntaje=Puntajes.get_worksheet(0)\n",
    "Puntaje_d = Puntaje.get_all_values()\n",
    "Puntaje_df=pd.DataFrame(Puntaje_d)\n",
    "Puntaje_df=Puntaje_df.iloc[2:,[0,27]]\n",
    "Puntaje_df.columns=[\"ID\",\"Participacion\"]\n",
    "pred_s[\"Puntaje\"] = pred_s[\"Clasificación\"].map(mapeo)\n",
    "pred_s=pred_s.iloc[:,[-2,-1]]\n",
    "last_row = len(Puntaje.get_all_values()) + 1 # Encuentra la primera fila vacía\n",
    "\n",
    "values = pred_s.values.tolist()\n",
    "Puntaje.update(f\"AB{last_row-len(pred_s)}\", values)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
